{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook\n",
    "tqdm_notebook().pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Read Default Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:\\\\Desktop\\\\MLPGD_Capstone_Project\\\\resources\\\\aaa_sample_data.xlsx'\n",
    "\n",
    "df_original = pd.read_excel(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data Glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* General Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.dtypes;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reading Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dict_member = 'D:\\\\Desktop\\\\MLPGD_Capstone_Project\\\\resources\\\\member_data_dict.xlsx'\n",
    "\n",
    "dict_member = pd.read_excel(path_dict_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dict_rodaside = 'D:\\\\Desktop\\\\MLPGD_Capstone_Project\\\\resources\\\\roadside_data_dict.xlsx'\n",
    "\n",
    "dict_roadside = pd.read_excel(path_dict_rodaside)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creating Dictionary to keep track of each operation and each filtering per operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_columns = {}\n",
    "removed_rows = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Dropping Erroneous Column (excel did that on converting the csv original file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1.1 - Defining the key for this operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_columns['Erroneous Columns Removal'] = ['Column1'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1.2 - Performing Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.drop(columns='Column1', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Grouping By Househol Key (main objective) and separate by variable type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.1 - Defining Data Type Columns and creating individual DataFrame per each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_columns = ['FSV CMSI Flag', 'FSV Credit Card Flag', 'FSV Deposit Program Flag', 'FSV Home Equity Flag', 'FSV ID Theft Flag', 'FSV Mortgage Flag', 'INS Client Flag', 'TRV Globalware Flag', 'Responded to Catalog', 'Mail Responder', 'Home Owner', 'Children', 'Gender', 'Bad Address Flag', 'Do Not Direct Mail Solicit', 'Email Available', 'Opt-Out - Publication', 'New Mover Flag', 'Occupant Type', 'Call Canceled', 'Call Killed', 'Cash Call', 'Fleet Indicator', 'Is Duplicate', 'Is NSR',\t'Member Match Flag', 'Was Duplicated',\t'Was Towed To AAR Referral'] # binary data that will be threat different from the rest of the numerical and categorical data\n",
    "\n",
    "binary = pd.DataFrame() # individual binary dataframe\n",
    "\n",
    "binary['Household Key'] = df_original['Household Key'] # concatenating household key column to perform groupby method\n",
    "\n",
    "numericals = pd.DataFrame() # individual numerical dataframe\n",
    "\n",
    "string = pd.DataFrame() # individual string dataframe\n",
    "\n",
    "string['Household Key'] = df_original['Household Key'] # concatenating household key column to perform groupby method\n",
    "\n",
    "dates = pd.DataFrame() # individual dates dataframe\n",
    "\n",
    "dates['Household Key'] = df_original['Household Key'] # concatenating household key column to perform groupby method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.2 - Filling each Individual Dataframe with the proper columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in tqdm_notebook(df_original.columns, desc='Process Progress'): # for all the columns\n",
    "\n",
    "    if c in binary_columns: # if the columns is in the binary categorie\n",
    "\n",
    "        binary[c] = df_original[c] # fill in the binary dataframe\n",
    "    \n",
    "    elif df_original.dtypes[c] in ['int64', 'float64']: # if the variables type of the given column are float or iteger\n",
    "\n",
    "        numericals[c] = df_original[c] # fill in the numerical dataframe\n",
    "\n",
    "    elif df_original.dtypes[c] in ['<M8[ns]']: # if the variables type of the given column are in the data format\n",
    "        \n",
    "        dates[c] = df_original[c] # fill in the date dataframe\n",
    "\n",
    "    else:\n",
    "\n",
    "        string[c] = df_original[c] # else, fill in the string dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.3 - Grouping by Household Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals = numericals.groupby('Household Key').mean() # grouping by the mean of numerical values per each Household Key\n",
    "\n",
    "binary_numericals = binary.groupby('Household Key').mean() # grouping by the mean of binary values per each Household Key We still need to separate categorical and numerical binary variables to perform the groupby procedure in the categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        2.3.1 - Converting Dates to Year and Separating Binary Numericals from Binary Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in tqdm_notebook(binary.columns, desc='Process Progress'): # Separating Binary Types\n",
    "    if (c != 'Household Key'): # we dont want to iterate over the Household Key unecessarily\n",
    "        if (binary.dtypes[c] in ['int64', 'float64']): # if the variables type of the given column are float or iteger \n",
    "\n",
    "            binary.drop(columns=c, inplace=True) # dropped from the future categorical binary dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in tqdm_notebook(dates.columns, desc='Process Progress'): # Converting Dates into Years\n",
    "    if (c != 'Household Key'):\n",
    "        dates[c] = dates[c].astype('datetime64[ns]')\n",
    "        dates[c] = dates[c].dt.year\n",
    "        #dates[c] = dates[c].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        2.3.2 - Grouping by mean and Household Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates  = dates.groupby('Household Key').mean() # grouping by the mean of year values per each Household Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        2.3.3 - Grouping by mode and Household Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = string.groupby('Household Key').apply(lambda x: x.mode()) # grouping by the mode of each categorical per each Household Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.head(n=2) # by some reason, the groupby method by the mode of string columns returned a multi-index dataframe. Therefore, we need to remove the multi-index of this dataframe to move further our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processed_string = pd.DataFrame() # creating a new dataframe for the post processed string variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for house_id in tqdm_notebook(df_original['Household Key'].unique().tolist(), desc='Process Progress'): # iterating over all unique Household Keys\n",
    "\n",
    "    if len(string.loc[house_id].index.tolist()) > 1: # if, for a given Household Key, we have more then one multi-index value\n",
    "\n",
    "        result = string.loc[house_id].iloc[0] + string.loc[house_id].iloc[1] # we sum the first two rows\n",
    "\n",
    "        non_null_indexes = np.where(string.loc[house_id].iloc[1].isnull().tolist())[0] # we evaluate which of them were not null before the sum procedure\n",
    "\n",
    "        for i in np.where(string.loc[house_id].iloc[1].isnull().tolist())[0]: # we iterate over the indexes that were not null before the sum procedure\n",
    "\n",
    "            result[i] = string.loc[house_id].iloc[0][i] # we replace by the previous non null value\n",
    "\n",
    "        post_processed_string = pd.concat([post_processed_string, result.to_frame().T]) # and we store on our new dataframe\n",
    "\n",
    "    else: # if, for a given Household Key, we have only one multi-index value\n",
    "\n",
    "        result = string.loc[house_id].iloc[0] # we store it because we can not change it so far\n",
    "\n",
    "        post_processed_string = pd.concat([post_processed_string, result.to_frame().T]) # # and we store on our new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processed_string = post_processed_string.set_index('Household Key') # set index to be sorted as Household Key values\n",
    "\n",
    "post_processed_string.index = post_processed_string.index.astype(int) # converting the index to iterger\n",
    "\n",
    "post_processed_string.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_categoricals = binary.groupby('Household Key').apply(lambda x: x.mode()) # grouping by the mode of each categorical per each Household Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_categoricals.head(n=5) # by some reason, the groupby method by the mode of binary_categoricals columns returned a multi-index dataframe. Therefore, we need to remove the multi-index of this dataframe to move further our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processed_binary_categoricals = pd.DataFrame() # creating a new dataframe for the post processed string variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for house_id in tqdm_notebook(df_original['Household Key'].unique().tolist(), desc='Process Progress'): # iterating over all unique Household Keys\n",
    "\n",
    "    if len(binary_categoricals.loc[house_id].index.tolist()) > 1: # if, for a given Household Key, we have more then one multi-index value\n",
    "\n",
    "        result = binary_categoricals.loc[house_id].iloc[0] + binary_categoricals.loc[house_id].iloc[1] # we sum the first two rows\n",
    "\n",
    "        non_null_indexes = np.where(binary_categoricals.loc[house_id].iloc[1].isnull().tolist())[0] # we evaluate which of them were not null before the sum procedure\n",
    "\n",
    "        for i in np.where(binary_categoricals.loc[house_id].iloc[1].isnull().tolist())[0]: # we iterate over the indexes that were not null before the sum procedure\n",
    "\n",
    "            result[i] = binary_categoricals.loc[house_id].iloc[0][i] # we replace by the previous non null value\n",
    "\n",
    "        post_processed_binary_categoricals = pd.concat([post_processed_binary_categoricals, result.to_frame().T]) # and we store on our new dataframe\n",
    "\n",
    "    else: # if, for a given Household Key, we have only one multi-index value\n",
    "\n",
    "        result = binary_categoricals.loc[house_id].iloc[0] # we store it because we can not change it so far\n",
    "\n",
    "        post_processed_binary_categoricals = pd.concat([post_processed_binary_categoricals, result.to_frame().T]) # # and we store on our new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processed_binary_categoricals = post_processed_binary_categoricals.set_index('Household Key') # set index to be sorted as Household Key values\n",
    "\n",
    "post_processed_binary_categoricals.index = post_processed_binary_categoricals.index.astype(int) # converting the index to iterger\n",
    "\n",
    "post_processed_binary_categoricals.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(numericals.index), len(binary_numericals.index), len(post_processed_binary_categoricals.index), len(dates.index), len(post_processed_string.index)) # number of rows per type of variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(numericals.columns), len(binary_numericals.columns), len(post_processed_binary_categoricals.columns), len(dates.columns), len(post_processed_string.columns)) # number of columns per type of variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Evaluating number of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_nullval_ratio = {} # dictionary of null values ration for all columns\n",
    "\n",
    "sequence = ['Numerical', 'Binary Numericals', 'Dates', 'Binary Categoricals', 'Strings'] # key sequence for dictionaries\n",
    "\n",
    "data_type_collection = [numericals, binary_numericals, dates, post_processed_binary_categoricals, post_processed_string] # list of dataframes per each data type\n",
    "\n",
    "for var_type, idx in zip(sequence, range(len(sequence))): \n",
    "\n",
    "    columns_nullval_ratio[var_type] = (data_type_collection[idx].isnull().sum() * 100 / len(data_type_collection[idx])).sort_values(ascending=False) # filling dataframe dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_type in sequence:\n",
    "\n",
    "    print(var_type +' Column Null Values','\\n', columns_nullval_ratio[var_type], '\\n') # extensive print, beware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3.1 - Removing columns beyond given threshold"
   ]
  },
  {
   "source": [
    "            3.1.1 - First Checkpoint"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_1 = {} # creating dictionary for the first checkpoint\n",
    "\n",
    "for var_type, idx in zip(sequence, range(len(sequence))): \n",
    "    checkpoint_1[var_type] = data_type_collection[idx].copy() # filling it with all types of dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            3.1.2 - Defining Column Threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_threshold = 40.0 # threshold of 40 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            3.1.3 - Defining the key for this operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key2 = str(column_threshold) + '% Column Threshold Removal' # key for our track removal dictionary\n",
    "\n",
    "removed_columns[key2] = [] # adding to the new key to the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            3.1.4 - Performing Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in tqdm_notebook(df_original.columns, desc='Process Progress'): # for all columns on the original dataframe\n",
    "    for var_type in sequence: # for all type of separatable dataframe \n",
    "\n",
    "        if (c == 'Occupation Code') or (c == 'Occupation Group'): # we want to preserve these two columns although we already now they would not pass the column threshold limit\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "\n",
    "            if c in checkpoint_1[var_type].columns: # if the column is in the list of columns of the given type of dataframe\n",
    "\n",
    "                if columns_nullval_ratio[var_type][c] >= column_threshold: # we check if the same does not satisfies the threshold limit\n",
    "\n",
    "                    checkpoint_1[var_type].drop(columns=c, inplace=True) # in this case we remove the given column from the given type of dataframe\n",
    "                \n",
    "                    removed_columns[key2].append(c) # and we add its name to the list of removed columns\n",
    "\n",
    "print('Removed Columns after filtering process: ', removed_columns[key2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_type, idx in zip(sequence, range(len(sequence))): # lets check the result now\n",
    "\n",
    "    columns_nullval_ratio[var_type] = (checkpoint_1[var_type].isnull().sum() * 100 / len(checkpoint_1[var_type])).sort_values(ascending=False) # filling dataframe dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for var_type in sequence:\n",
    "\n",
    "    #print(var_type +' Column Null Values','\\n', columns_nullval_ratio[var_type], '\\n') # Nice! We have dropped the columns with a lot of null information that would make difficult for us to predict any of them "
   ]
  },
  {
   "source": [
    "    3.2 - Removing rows (Household Keys) beyond given threshold"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        3.2.1 - Global Mean of Null Values for all Household Keys available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_nullval_ratio = {} # dictionary of null values sum for all columns\n",
    "\n",
    "for var_type in sequence: \n",
    "\n",
    "    rows_nullval_ratio[var_type] = (checkpoint_1[var_type].isnull().sum(axis=1).sort_values(ascending=False)) # filling dataframe dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_num_null_rows = [] # list of sum of all null values per row\n",
    "\n",
    "for var_type in sequence:\n",
    "\n",
    "    sum_num_null_rows.append(round(rows_nullval_ratio[var_type].sum())) # filling list\n",
    "\n",
    "total_mean = round(sum(sum_num_null_rows)/len(rows_nullval_ratio['Numerical'].index.tolist())) # general mean of null values per row\n",
    "\n",
    "print('\\n','Mean Number of Null values on Rows:', total_mean) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        3.2.2 - Creating Key and Dictionary for this operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_rows = {}\n",
    "\n",
    "row_key1 = str(total_mean) + ' Number of Null per Row Threshold Removal' # key for our track removal dictionary\n",
    "\n",
    "removed_rows[row_key1] = [] # adding to the new key to the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        3.2.3 - Global Mean of each Household Key available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_mean = {} # global mean number of null values for each Household Key\n",
    "\n",
    "for house_id in tqdm_notebook(checkpoint_1[sequence[0]].index.tolist(), desc='Process Progress'): # for all unique Household Keys on the original dataframe\n",
    "    tmp = 0 # temporary value for each global mean of the given Household Key\n",
    "    for var_type in sequence: # for all type of separatable dataframe \n",
    "\n",
    "        tmp += rows_nullval_ratio[var_type][house_id]\n",
    "\n",
    "    house_mean[house_id] =  tmp.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        3.2.4 - Perform Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for house_id in tqdm_notebook(checkpoint_1[sequence[0]].index.tolist(), desc='Process Progress'): # for all unique Household Keys on the original dataframe\n",
    "    for var_type in sequence: # for all type of separatable dataframe        \n",
    "        if house_mean[house_id] >= total_mean: # we check if the same does not satisfies the threshold limit\n",
    "\n",
    "            for var_type in sequence: # for all type of separatable dataframe \n",
    "\n",
    "                if house_id in checkpoint_1[var_type].index.tolist(): # check if Household Key has not been already droped to prevent error raise\n",
    "\n",
    "                    checkpoint_1[var_type].drop(house_id, inplace=True) # drop the row related to the given Household Key in all dataframes type\n",
    "\n",
    "                    removed_rows[row_key1].append(house_id) # and we add its name to the list of removed columns\n",
    "\n",
    "print('Removed Rows after filtering process: ', removed_rows[row_key1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_type, idx in zip(sequence, range(len(sequence))): # lets check the result now\n",
    "\n",
    "    rows_nullval_ratio[var_type] = (checkpoint_1[var_type].isnull().sum(axis=1).sort_values(ascending=False)) # filling dataframe dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_type in sequence:\n",
    "\n",
    "    print(var_type + ':', len(checkpoint_1[var_type].index)) # total number of rows in each type of dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Subjective Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4.1 - Rename FSV, INS, and TRV columns that are related to products purchase flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_FSV = [s for s in list(checkpoint_1['Binary Categoricals'].columns) if \"FSV\" in s] # creating a list of the columns that have at least the FSV mnemonic\n",
    "matching_INS = [s for s in list(checkpoint_1['Binary Categoricals'].columns) if \"INS\" in s] # creating a list of the columns that have at least the INS mnemonic\n",
    "matching_TRV = [s for s in list(checkpoint_1['Binary Categoricals'].columns) if \"TRV\" in s] # creating a list of the columns that have at least the TRV mnemonic\n",
    "final_matching = matching_FSV + matching_INS + matching_TRV # concatenating them all\n",
    "print(final_matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1 # counter flag\n",
    "\n",
    "for c in final_matching:\n",
    "\n",
    "    checkpoint_1['Binary Categoricals'].rename(columns={c : 'Purchased Product' + ' ' + str(counter)}, inplace=True) # renaming the columns\n",
    "\n",
    "    counter = counter + 1 # index of the new product column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_1['Binary Categoricals'].head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4.2 -  Removing columns that are not allowed or not ethical to work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.2.1 - Creating Dictionary Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key3 = 'Unethical Columns' # creating key to removed columns\n",
    "\n",
    "removed_columns[key3] = ['Race', 'Language', 'Gender'] # since we are talking about Households, the same can not be differentiate by gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.2.2 - Performing Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in tqdm_notebook(removed_columns[key3], desc='Process Progress'): # iterating over the given columns\n",
    "    for var_type in sequence: # for each dataframe type\n",
    "        if c in checkpoint_1[var_type]: # if the columns is in the dataframe\n",
    "\n",
    "            checkpoint_1[var_type].drop(columns=c, inplace=True) # we dropped\n",
    "\n",
    "print('Removed Columns after filtering process: ', removed_columns[key3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4.3 - Removing Columns that represent the same information in a different way (information redundancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.3.1 - Creating Dictionary Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key4 = 'Redundancy Information Columns' # generating key for this filtering procedure\n",
    "\n",
    "removed_columns[key4] = ['Individual Key', 'State - Grouped', 'ZIP5', 'ZIP9', 'Children', 'Birth Date MMDDYYYY', 'Cancel Date', 'County', 'Do Not Direct Mail Solicit', 'Right_Individual Key', 'Member Key', 'Member Number Associate ID', 'Membership ID', 'Reinstate Date', 'ZIP', 'Mosaic Household', 'kcl_B_IND_MosaicsGrouping', 'Occupation Code', 'Breakdown State', 'Call Killed', 'Clearing Code Last Description', 'Dispatch Code1 Description','DTL Prob1 Code Description', 'Is Duplicate', 'Member Match Flag', 'Member Number and Associate ID', 'SC Date', 'Rec ID', 'SC STS RSN Code Description', 'SC Vehicle Model Name', 'SVC Facility Name', 'SVC Facility Type', 'Tow Destination Latitude', 'Tow Destination Longitude', 'Member Map Location'] #  Comlumns ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.3.2 - Performing Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in tqdm_notebook(removed_columns[key4], desc='Process Progress'): # for each column in the redundancy columns list\n",
    "    for var_type in sequence: # for each dataframe type\n",
    "        if c in checkpoint_1[var_type]: # if the columns is in the given dataframe\n",
    "            checkpoint_1[var_type].drop(columns=c, inplace=True) # we dropped\n",
    "\n",
    "print('Removed Columns after filtering process: ', removed_columns[key4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_type in sequence:\n",
    "\n",
    "    print(var_type + ':', len(checkpoint_1[var_type].columns)) # number of remaining columns for each dataframe type after the filtering process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4.4 - Checking Columns with low percentage of variantional information (counting null values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.4.1 - Creating Dictionay Key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key5 = 'Low Variational Information Columns' # generating key for this filtering procedure\n",
    "\n",
    "removed_columns[key5] = [] # generating empty list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.4.2 - Performing Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_type in tqdm_notebook(sequence, desc='Process Progress'): # for every dataframe type\n",
    "    for c in checkpoint_1[var_type].columns: # for every column in the given dataframe\n",
    "\n",
    "        if checkpoint_1[var_type].nunique()[c] == 1: # we check if the values on the given column are all the same\n",
    "\n",
    "            checkpoint_1[var_type].drop(columns=c, inplace=True) # if it is so, we drop the column\n",
    "\n",
    "            removed_columns[key5].append(c) # and we fill the removed columns list\n",
    "        \n",
    "print('Removed Columns after filtering process: ', removed_columns[key5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_type in sequence:\n",
    "\n",
    "    print(var_type + ':', len(checkpoint_1[var_type].columns)) # number of remaining columns for each dataframe type after the filtering process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4.5 - Removing Columns with Unnecessary Information (extremely subjective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.5.1 - Creating Dictionary Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key6 = 'Unnecessary Columns' # generating key for this filtering procedure\n",
    "\n",
    "removed_columns[key6] = ['Address Change Date', 'Bad Address Flag', 'Billing Code Description', 'Join Club Date', 'Member Phone Type', 'Mosaic Global Household', 'New Mover Flag', 'Call Canceled', 'Is NSR', 'Plus Indicator Description', 'Was Duplicated', 'Was Towed To AAR Referral', 'Branch Name', 'Member Map Location', 'Breakdown Map Location'] #  Comlumns ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.5.2 - Performing Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in tqdm_notebook(removed_columns[key6], desc='Process Progress'): # for each column in the unncessary columns list\n",
    "    for var_type in sequence: # for each dataframe type\n",
    "\n",
    "        if c in checkpoint_1[var_type]: # if the column is in the given dataframe\n",
    "            checkpoint_1[var_type].drop(columns=c, inplace=True) # we dropped\n",
    "\n",
    "print('Removed Columns after filtering process: ', removed_columns[key6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_type in sequence:\n",
    "\n",
    "    print(var_type + ':', len(checkpoint_1[var_type].columns)) # number of remaining columns for each dataframe type after the filtering process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Summarize Information and Fill Nulls in the given columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Saving checkpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_2 = {} # creating dictionary for the second checkpoint\n",
    "\n",
    "for var_type, idx in zip(sequence, range(len(sequence))): \n",
    "    checkpoint_2[var_type] = checkpoint_1[var_type].copy() # filling in with all dataframe types"
   ]
  },
  {
   "source": [
    "        4.1 - Dealing first with the Numericals Dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_2[sequence[0]].head(n=3) # lets remember what variables and how do they behave on this dataframe"
   ]
  },
  {
   "source": [
    "                4.1.1 - Lets take a look at the Length Of Residence variable"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_2[sequence[0]]['Length Of Residence'].unique() # total number of null values for the Length Of Residence variable"
   ]
  },
  {
   "source": [
    "                    4.1.1.1 - Round the values and converting them to interger type to prevent any error"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in tqdm_notebook(checkpoint_2[sequence[0]]['Length Of Residence'].index, desc='Process Progress'): # for all indexes of the given dataframe\n",
    "\n",
    "    checkpoint_2[sequence[0]].at[idx, 'Length Of Residence'] = round(checkpoint_2[sequence[0]]['Length Of Residence'][idx]) # replace de Null value by the Household Key mean\n",
    "\n",
    "    if np.isnan(checkpoint_2[sequence[0]]['Length Of Residence'][idx]):\n",
    "\n",
    "        checkpoint_2[sequence[0]].at[idx, 'Length Of Residence'] = 0\n",
    "\n",
    "checkpoint_2[sequence[0]]['Length Of Residence'] = checkpoint_2[sequence[0]]['Length Of Residence'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(checkpoint_2[sequence[0]]['Length Of Residence'].tolist());"
   ]
  },
  {
   "source": [
    "    As we can see, we still have a considerable amount of null values in the Length Of Residence variable. If we replaced these values by the mean of the variable we would be disregarding the geographical relation. Therefore, we will use the mean related to the City variable to predict the null values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_2['Strings']['City'].isnull().sum() # number of Household Keys with null City values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_2[sequence[0]]['City'] = checkpoint_2['Strings']['City'].values # temporarily concatenate the city column on the numerical column"
   ]
  },
  {
   "source": [
    "                4.1.1.2 - Luckly, we can rely on the City variable, stored at the Strings dataframe. Thus, we can proceed with the Null values prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in tqdm_notebook(checkpoint_2[sequence[0]].index, desc='Process Progress'): # For all Household Keys\n",
    "\n",
    "    if checkpoint_2[sequence[0]]['Length Of Residence'][idx] == 0: # we check if the Length of Residence variable is equal zero (previouly changed from nan values)\n",
    "\n",
    "        tmp_city = checkpoint_2[sequence[0]]['City'][idx] # we store a temporary value of the city for the given Household Key\n",
    "\n",
    "        checkpoint_2[sequence[0]]['Length Of Residence'][idx] = checkpoint_2[sequence[0]]['Length Of Residence'].loc[checkpoint_2[sequence[0]]['City'] == tmp_city].values.mean() # replacing zeroed value by mean of the Length of Residence per grouped by City"
   ]
  },
  {
   "source": [
    "                4.1.1.3 - Droping City variable from Numerical dataframe type"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_2[sequence[0]].drop(columns='City', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.1.2 - Rename ERS ENT columns that are related to number of roadside calls and creating a column that presents the sum of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            4.1.2.1 Finding given columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_ERS_ENT = [s for s in list(checkpoint_2[sequence[0]].columns) if \"ERS ENT\" in s] # creating a list of the columns that have at least the ERS ENT mnemonic\n",
    "\n",
    "print(matching_ERS_ENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            4.1.2.2 Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1 # counter flag\n",
    "\n",
    "for c in matching_ERS_ENT:\n",
    "\n",
    "    checkpoint_2[sequence[0]].rename(columns={c : 'Number of Roadside calss for Year' + ' ' + str(counter)}, inplace=True) # renaming the columns\n",
    "\n",
    "    counter = counter + 1 # index of the new product column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_2[sequence[0]].head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            4.1.2.3 - Calculating Total Amount of Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column = [] # creating list for the new column to be appended\n",
    "\n",
    "for idx in tqdm_notebook(checkpoint_2[sequence[0]].index, desc='Process Progress'): # For each row\n",
    "    \n",
    "    total_amount = 0 # total amount of calls for each Household Key\n",
    "\n",
    "    for c in ['Number of Roadside calss for Year 1', 'Number of Roadside calss for Year 2',\t'Number of Roadside calss for Year 3']: # for each Number of Roadside calls for each Year\n",
    "\n",
    "        if np.isnan(checkpoint_2[sequence[0]][c][idx]): # checking for nan values existance\n",
    "\n",
    "            checkpoint_2[sequence[0]].at[idx, c] = 0 # replacing them by zero calls\n",
    "                  \n",
    "        checkpoint_2[sequence[0]].at[idx, c] = round(checkpoint_2[sequence[0]][c][idx]) # rounding the call values\n",
    "\n",
    "        checkpoint_2[sequence[0]][c] = checkpoint_2[sequence[0]][c].astype(int) # converting the final values to iterger type\n",
    "\n",
    "        total_amount += checkpoint_2[sequence[0]][c][idx] # summing the total amount of call\n",
    "\n",
    "    new_column.append(total_amount) # appending to list representing the new column to be appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = 'Total Number of Roadside Calls' # naming the new column\n",
    "\n",
    "checkpoint_2[sequence[0]][column_name] = new_column # appendding new column\n",
    "\n",
    "checkpoint_2[sequence[0]][column_name] = checkpoint_2[sequence[0]][column_name].astype(int) # converting to iterger type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_2[sequence[0]].head(n=2)"
   ]
  },
  {
   "source": [
    "## (Exploratory Data Analysis - EDA)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Purchase Probability Prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Clustering By Household Types"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4.1 - Number of Purchased Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.1.1 - Creating Dictionary Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key7 = 'Merging total number of purchased products by each row' # generating key for this filtering procedure\n",
    "\n",
    "removed_columns[key7] = ['Purchased Product 1',\t'Purchased Product 2',\t'Purchased Product 3',\t'Purchased Product 4',\t'Purchased Product 5',\t'Purchased Product 6',\t'Purchased Product 7',\t'Purchased Product 8'] # column IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.1.2 - Performing Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            4.1.2.1 - Converting String Values in Numerical Values and filling Null values as 0 purchased products for empty different Household Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in tqdm_notebook(removed_columns[key7], desc='Process Progress'): # In each Purchased Product Column\n",
    "    for var_type in sequence: # for each dataframe type\n",
    "        for idx in range(len(checkpoint_2[var_type])): # For each row\n",
    "\n",
    "            if c in checkpoint_2[var_type].columns: # if the column belongs to the dataframe\n",
    "\n",
    "                if (checkpoint_2[var_type][c].values[idx] == 'N') or (checkpoint_2[var_type][c].values[idx] == 'Null') or (checkpoint_2[var_type][c].values[idx] == np.nan): # if the value of the given column on the given index is N (No), Null or Nan\n",
    "\n",
    "                    checkpoint_2[var_type][c].values[idx] = 0 # set the new value to zero\n",
    "\n",
    "                else:\n",
    "\n",
    "                    checkpoint_2[var_type][c].values[idx] = 1 # else, set the new value to one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            4.1.2.2 - Converting Columns to Iterger Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in tqdm_notebook(removed_columns[key7], desc='Process Progress'):\n",
    "    for var_type in sequence: # for each dataframe type\n",
    "        if c in checkpoint_2[var_type].columns: # if the column belongs to the dataframe\n",
    "\n",
    "            checkpoint_2[var_type][c] = checkpoint_2[var_type][c].astype(int) # converting the final values to iterger type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.1.2.3 - Summarize all Purchased Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column = [] # creating list for the new column to be appended\n",
    "\n",
    "column_name = 'Total Number of Purchased Products' # naming the new column\n",
    "\n",
    "for var_type in tqdm_notebook(sequence, desc='Process Progress'): # for each dataframe type\n",
    "    for idx in checkpoint_2[var_type].index: # For each row\n",
    "        total_amount = 0 # total amount of products bought in each row\n",
    "\n",
    "        if c in checkpoint_2[var_type].columns: # if the column belongs to the dataframe\n",
    "\n",
    "            for c in removed_columns[key7]: # In each Purchased Product Column\n",
    "\n",
    "                if checkpoint_2[var_type][c][idx] == 1:\n",
    "\n",
    "                    total_amount = total_amount + 1\n",
    "\n",
    "            new_column.append(total_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.1.2.4 - Drop Separate Purchased Data and Concatenate Total Purchased Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in tqdm_notebook(removed_columns[key7], desc='Process Progress'):\n",
    "    for var_type in sequence: # for each dataframe type\n",
    "        if c in checkpoint_2[var_type].columns: # if the column belongs to the dataframe\n",
    "\n",
    "            checkpoint_2[var_type].drop(columns=c, inplace=True)\n",
    "\n",
    "            flag = var_type\n",
    "\n",
    "checkpoint_2[sequence[0]][column_name] = new_column\n",
    "\n",
    "\n",
    "\n",
    "print('Removed Columns after filtering process: ', removed_columns[key7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_type in sequence:\n",
    "\n",
    "    print(var_type + ':', len(checkpoint_2[var_type].columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4.2 - Correcting Number of Children column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.2.2 - Converting String Values in Numerical Values and filling Null values as 0 children for empty different Household Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_type in sequence:\n",
    "\n",
    "    if 'Number of Children' in checkpoint_2[var_type].columns:\n",
    "\n",
    "        mask_n_children = checkpoint_2[var_type]['Number of Children'].isnull() # creating mask of null values in Number of Children column\n",
    "\n",
    "        flag = var_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in tqdm_notebook(checkpoint_2[flag].index, desc='Process Progress'): # For each row\n",
    "\n",
    "    if mask_n_children[idx]: # if the mask is true, that means that the correponded value for that index is Null\n",
    "\n",
    "         checkpoint_2[var_type]['Number of Children'].values[idx] = 0    \n",
    "    \n",
    "    else:\n",
    "\n",
    "        entry =  checkpoint_2[var_type]['Number of Children'].values[idx].split()[0]\n",
    "\n",
    "        if (entry == 'No'):\n",
    "\n",
    "             checkpoint_2[var_type]['Number of Children'].values[idx] = 0\n",
    "\n",
    "        elif (entry == 'One'):\n",
    "\n",
    "             checkpoint_2[var_type]['Number of Children'].values[idx] = 1\n",
    "\n",
    "        elif (entry == 'Two'):\n",
    "\n",
    "             checkpoint_2[var_type]['Number of Children'].values[idx] = 2\n",
    "\n",
    "        elif (entry == 'Three'):\n",
    "\n",
    "             checkpoint_2[var_type]['Number of Children'].values[idx] = 3\n",
    "\n",
    "        elif (entry == 'Four'):\n",
    "\n",
    "             checkpoint_2[var_type]['Number of Children'].values[idx] = 4\n",
    "\n",
    "        elif (entry == 'Five'):\n",
    "\n",
    "             checkpoint_2[var_type]['Number of Children'].values[idx] = 5\n",
    "\n",
    "        elif (entry == 'Six'):\n",
    "\n",
    "             checkpoint_2[var_type]['Number of Children'].values[idx] = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.2.2 - Converting Column to Iterger Type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data['Number of Children'] = filtered_data['Number of Children'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4.3 - Length of Residence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The main approach we are going to apply here is to get the mean value of the household key and apply it on the same number. After that, if it still exists any null value on the given column, we will use the mean or the mode of the column City to fill it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.3.1 - Replace random default value apllied by Pandas on Null values for the Length Of Residence column. For some reason it was choosen a smaller number as possible for a float64 number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data['Length Of Residence'].unique()  # as we can see, by some reason, the number -9.223372e+18 was choosen to be a flag of a Null value for this column, we need to replace it by 0 or np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data['Length Of Residence'] = filtered_data['Length Of Residence'].replace(filtered_data['Length Of Residence'].unique()[0], np.nan) # replacing random numerical Null values chose by Pandas for numpy NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data['Length Of Residence'].unique() # now we have out correct output for Null values for the Length Of Residence column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.3.2 - Checking if there are Household Keys with at least one value for the Length Of Residence column and apply it to the rest of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.groupby('Household Key')['Length Of Residence'].unique(); # As we can see, for the same Household Key level we have different values of Length Of Residence. This happens because our main sheet data was sorted on a Individual Key level, that is not our interest here. # OBS: REMOVE THE ';' AT THE END OF THE COMMAND TO PRINT THE OUTPUT! BE ADVISED, THE OUTPUT IS EXTENSE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for house_id in tqdm_notebook(filtered_data.groupby('Household Key')['Length Of Residence'].unique().keys(), desc='Process Progress'): # for every single unique Household Key value\n",
    "\n",
    "    if len(filtered_data.groupby('Household Key')['Length Of Residence'].unique()[house_id]) == 1: # if there is only one associated value for Length Of Residence, independet of the value, we can infer nothing for now, so, we pass\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "\n",
    "        tmp = filtered_data.loc[filtered_data['Household Key'] == house_id]['Length Of Residence'].values # storing array on a temporary file\n",
    "\n",
    "        if np.isnan(tmp).any(): # check if the associated values for the Length Of Residence for a given Household Key has at least one Null value\n",
    "\n",
    "            nan_positions = np.where(np.isnan(tmp))[0].tolist() # grabing the positions where the Null values exist\n",
    "\n",
    "            if len(nan_positions) == len(tmp): # checking if all the values for the given Household Key are Null (in this case we will treat it in another way, so, we pass)\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "\n",
    "                replacement_value =  tmp[~np.isnan(tmp)].mean() # calculating the mean of all non Null values\n",
    "\n",
    "                tmp[np.isnan(tmp)] = replacement_value # index of Null values on the array\n",
    "\n",
    "                idxs = filtered_data.loc[filtered_data['Household Key'] == house_id, 'Length Of Residence'].index[nan_positions] # index of Null values on the dataframe\n",
    "\n",
    "                for i in idxs: # for each dataframe index\n",
    "                \n",
    "                    filtered_data.at[i, 'Length Of Residence'] = replacement_value # replace de Null value by the Household Key mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.3.3 - Now we focus on predicting the rest of the Null values, but this time through groups of cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.groupby('City')['Length Of Residence'].unique(); # As we can see, for the same City we have different values of Length Of Residence. We want to take advantage of this bigger scale to replace more Null values # OBS: REMOVE THE ';' AT THE END OF THE COMMAND TO PRINT THE OUTPUT! BE ADVISED, THE OUTPUT IS EXTENSE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in tqdm_notebook(filtered_data.groupby('City')['Length Of Residence'].unique().keys(), desc='Process Progress'): # for every single unique Household Key value\n",
    "\n",
    "    if len(filtered_data.groupby('City')['Length Of Residence'].unique()[city]) == 1: # if there is only one associated value for Length Of Residence, we can infer nothing for now. So, we pass and let this for the state mean \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "\n",
    "        tmp = filtered_data.loc[filtered_data['City'] == city]['Length Of Residence'].values # storing array on a temporary file\n",
    "\n",
    "        if np.isnan(tmp).any(): # check if the associated values for the Length Of Residence for a given City has at least one Null value\n",
    "\n",
    "            nan_positions = np.where(np.isnan(tmp))[0].tolist() # grabing the positions where the Null values exist\n",
    "\n",
    "            if len(nan_positions) == len(tmp): # checking if all the values for the given City are Null (in this case we will treat it in another way, so, we pass)\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "\n",
    "                replacement_value =  tmp[~np.isnan(tmp)].mean() # calculating the mean of all non Null values\n",
    "\n",
    "                tmp[np.isnan(tmp)] = replacement_value # index of Null values on the array\n",
    "\n",
    "                idxs = filtered_data.loc[filtered_data['City'] == city, 'Length Of Residence'].index[nan_positions] # index of Null values on the dataframe\n",
    "\n",
    "                for i in idxs: # for each dataframe index\n",
    "                \n",
    "                    filtered_data.at[i, 'Length Of Residence'] = replacement_value # replace de Null value by the City mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            4.3.4 - Now we focus on predicting the rest of the Null values on the biggest and final scale as possible, the states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            Note: You may noticed that we have excluded the State - Gouped column, but, since we havent eliminated any row yet we can still use the previous indexes of this unusable column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_idxs = df_original.index[df_original['State - Grouped'] == 'CT'].tolist() # grab index of all rows that belong to the CT state\n",
    "ri_idxs = df_original.index[df_original['State - Grouped'] == 'RI'].tolist() # grab index of all rows that belong to the RI state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_mean = filtered_data['Length Of Residence'][ct_idxs].mean()\n",
    "ri_mean = filtered_data['Length Of Residence'][ri_idxs].mean()\n",
    "print('CT mean value considering all Null values so far filled: ', ct_mean, '\\n', 'RI mean value considering all Null values so far filled: ', ri_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in tqdm_notebook(filtered_data.index, desc='Process Progress'): # for every single unique Household Key value\n",
    "\n",
    "    if filtered_data['Length Of Residence'].isnull()[idx]: # if for the given index of the Length Of Residence column the value is Null\n",
    "        if idx in ct_idxs: # check if this index belongs to the CT state\n",
    "\n",
    "            filtered_data.at[idx, 'Length Of Residence'] = ct_mean # replace the Null value by the CT state mean\n",
    "\n",
    "        else:\n",
    "\n",
    "            filtered_data.at[idx, 'Length Of Residence'] = ri_mean # replace the Null value by the RI state mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data['Length Of Residence'].isnull().values.any() # check if Length Of Residence column still has any Null value left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4.4 - Summarize E-mail and Mail Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.4.1 - Creating List of Columns to be summarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mail_var = ['Mail Responder', 'Email Available', 'Email Status', 'Opt-Out - Publication']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.4.2 - Checking Existance of Null Values on this columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in mail_var:\n",
    "\n",
    "    print('Flag if column ', col, ' has null values. ', filtered_data[col].isnull().values.any(), ' and its percentage ', nullval_ratio[col]) # check if the columns have any Null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        As we can see, all the binary categorical columns have Null values. However, the columns related to Email Available and Opt-Out - Publication are almost full filled and, according to the spreadsheet dictionary, those two are AAA direct information and not third party information. So, we will focus on them as most reliable data and use the Member Status as well to \"predict\" a summary of all these information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.groupby('Household Key')['Email Status'].unique();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.4.3 - Give the name of the new column to be added on the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mail_key = 'Email Correspondent' # new column name\n",
    "\n",
    "filtered_data[new_mail_key] = \"\" # appending new column to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.4.4 - To summarize all these information we need to ensure that for the same Household Key we have a concordance between all the variables. For that we will use the Member Status variable and Email Available as pivot information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for house_id in tqdm_notebook(filtered_data['Household Key'].unique().tolist(), desc='Process Progress'): # for every single unique Household Key value\n",
    "\n",
    "    if len(Counter(filtered_data.groupby('Household Key')['Member Status'].unique()[house_id].tolist())) != 1: # if there is more than one Member Status type for the given Household Key\n",
    "\n",
    "        if 'ACTIVE' in filtered_data.groupby('Household Key')['Member Status'].unique()[house_id].tolist(): # if at least one individual member is active on the given HouseHold Key\n",
    "\n",
    "            filtered_data.loc[(filtered_data['Household Key'] == house_id), 'Member Status'] = 'ACTIVE' # replace them all for active\n",
    "\n",
    "        elif 'PENDING' in filtered_data.groupby('Household Key')['Member Status'].unique()[house_id].tolist(): # if at least one individual member is pending on the given HouseHold Key and there is none active\n",
    "\n",
    "            filtered_data.loc[(filtered_data['Household Key'] == house_id), 'Member Status'] = 'PENDING' # replace them all for pending \n",
    "    \n",
    "    if len(Counter(filtered_data.groupby('Household Key')['Email Available'].unique()[house_id].tolist())) != 1: # if there is more than one Email Available type for the given Household Key\n",
    "\n",
    "        if 10 in filtered_data.groupby('Household Key')['Email Available'].unique()[house_id].tolist(): # if at least one individual member has its email available on the given HouseHold Key\n",
    "\n",
    "            filtered_data.loc[(filtered_data['Household Key'] == house_id), 'Email Available'] = 10 # replace them all by 10 (active)\n",
    "\n",
    "    if len(Counter(filtered_data.groupby('Household Key')['Opt-Out - Publication'].unique()[house_id].tolist())) != 1: # if there is more than one Opt-Out - Publication type for the given Household Key   \n",
    "\n",
    "        if 'Opt-In' in filtered_data.groupby('Household Key')['Opt-Out - Publication'].unique()[house_id].tolist(): # if at least one individual member has optional for an email publication on the given HouseHold Key\n",
    "\n",
    "            filtered_data.loc[(filtered_data['Household Key'] == house_id), 'Opt-Out - Publication'] = 'Opt-In' # replace them all by Opt-In (active)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.4.5 - Filling the new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for house_id in tqdm_notebook(filtered_data['Household Key'].unique().tolist(), desc='Process Progress'): # for every single unique Household Key value\n",
    "\n",
    "    if ('ACTIVE' in filtered_data[filtered_data['Household Key'] == house_id]['Member Status'].tolist()) and (10 in filtered_data[filtered_data['Household Key'] == house_id]['Email Available'].tolist()) and ('Opt-In' in filtered_data[filtered_data['Household Key'] == house_id]['Opt-Out - Publication'].tolist()): # if all the conditions are satisfied\n",
    "\n",
    "        filtered_data.loc[(filtered_data['Household Key'] == house_id), 'Email Correspondent'] = 1 # the household is an e-mail correspondent\n",
    "\n",
    "    else:\n",
    "\n",
    "        filtered_data.loc[(filtered_data['Household Key'] == house_id), 'Email Correspondent'] = 0 # the household is not an e-mail correspondent\n",
    "\n",
    "\n",
    "filtered_data['Email Correspondent'] = filtered_data['Email Correspondent'].astype(int) # changing to integer data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.4.6 -  Dropping Email information Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key8 = 'Merging Availability of Email Contact'\n",
    "\n",
    "removed_columns[key8] = mail_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in mail_var:\n",
    "\n",
    "    filtered_data.drop(columns=c, inplace=True)\n",
    "\n",
    "print('Removed Columns after filtering process: ', removed_columns[key8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = '..\\\\resources\\\\checkpoint.csv'\n",
    "filtered_data.to_csv(path_or_buf=path_out, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        We still need to keep an eye on the variables that only have 2 unique values, but it will be more a subjective evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#correlation matrix\n",
    "corrmat = remove_null_data.corr()\n",
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "sns.heatmap(corrmat, square=True); # considering 70% o correlation as minimum to show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  1- There is an \"island\" between Rec ID and Tow destination Longitude this will be evaluate latter, but looks promissing\n",
    "  2- Basic Cost has a high correlation with the variables within this \"island\" \n",
    "  3- Individual Key and Right_Individual Key are pratically the same variable\n",
    "  4- ZIP5 and ZIP9 are pratically the same variable\n",
    "  5- Months from join to Cancel has no correlation at so ever with the Premier Cost variable\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    By the correlation matrix of the entire dataset (minus the columns with more than 70% of null values), we can see that there are still some columns with no correlation, or total correlation with all the variables, and some trouble columns. Therefore, the best thing to do is to drop them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trouble_columns= ['Is Duplicate', 'Member Match Flag']\n",
    "\n",
    "columns_will_not_use = ['Individual Key', 'Member Flag', 'Right_Individual Key']"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitcapstoneprojectcondad07c177c475741d8a46606514fa38a7a",
   "display_name": "Python 3.7.6 64-bit ('capstone_project': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}